{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import requests\n",
    "from time import time\n",
    "import time\n",
    "\n",
    "import base64\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'timeout': 360, 'maxsize': 25}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'myandex'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_by_id = {}\n",
    "es.indices.delete(index='myandex')\n",
    "es.indices.create(index='myandex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFile(i):\n",
    "    prefix = '../byweb_for_course/byweb.'\n",
    "    suffix = '.xml'\n",
    "    filename = prefix + str(i) + suffix\n",
    "    with open(filename, 'rb') as f:\n",
    "        decoded = f.read().decode('cp1251')\n",
    "        xmldict = xmltodict.parse(decoded)\n",
    "        for doc in tqdm(xmldict['romip:dataset']['document']):\n",
    "            try:\n",
    "                docID = doc['docID']\n",
    "                documents_by_id[docID] = {}\n",
    "                url = base64.b64decode(doc['docURL']).decode('cp1251')\n",
    "                content = base64.b64decode(doc['content']['#text']).decode('cp1251')\n",
    "                documents_by_id[docID]['url'] = url\n",
    "                documents_by_id[docID]['content'] = content\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178779acf45b41ebbf5320ab7949deb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1): # FIX\n",
    "    processFile(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_final = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'url': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'content': {\n",
    "                'type': 'text',\n",
    "                \"analyzer\": \"my_custom_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"my_custom_analyzer\": {\n",
    "          \"type\":      \"custom\", \n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"char_filter\": [\n",
    "            \"html_strip\",\n",
    "            \"yont\"\n",
    "          ],\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            #\"asciifolding\",\n",
    "            \"russian_snow\",\n",
    "            \"english_snow\"\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "        'char_filter': {\n",
    "                'yont': {\n",
    "                    'type': 'mapping',\n",
    "                    'mappings': [\n",
    "                        'ё => е',\n",
    "                        'Ё => Е'\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "    'filter': {\n",
    "            'stop_words': {\n",
    "                'type': 'stop',\n",
    "                'stopwords': [\n",
    "                ]\n",
    "            },\n",
    "            'russian_snow': {\n",
    "                'type': 'snowball',\n",
    "                'language': 'russian'\n",
    "            },\n",
    "            'english_snow': {\n",
    "                'type': 'snowball',\n",
    "                'language': 'english'\n",
    "            }\n",
    "     }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_index():\n",
    "    es.indices.delete(index='myandex')\n",
    "    es.indices.create(index='myandex', body=settings_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "recreate_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_analyzer(analyzer, text):\n",
    "    body = analyzer\n",
    "    body['text'] = text\n",
    "    \n",
    "    tokens = es.indices.analyze(index='myandex', body=body)['tokens']\n",
    "    tokens = [token_info['token'] for token_info in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bla', 'bla', 'русск', 'countabl', 'текст', 'ешкин', 'кот']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = {\n",
    "    'analyzer': 'my_custom_analyzer'\n",
    "}\n",
    "\n",
    "check_analyzer(analyzer, '<meta http-equiv=\"Content-Type\" content=\"text/html; charset=windows-1251\"> bla bla русский countable текст Ёшкин кот')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_es_action(index, doc_id, document):\n",
    "    return {\n",
    "        '_index': index,\n",
    "        '_id': doc_id,\n",
    "        '_source': document\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_actions_generator():\n",
    "    for doc_id, doc in tqdm(documents_by_id.items()):\n",
    "        yield create_es_action('myandex', doc_id, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f86883012704a09be3c4c6b6440bc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time on index creation: 00:00:01.12\n",
      "In seconds: 1.4075696468353271\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for ok, result in parallel_bulk(es, es_actions_generator(), queue_size=4, thread_count=4, chunk_size=1000):\n",
    "    if not ok:\n",
    "        print(result)\n",
    "end = time.time()\n",
    "print(f\"Time on index creation: {time.strftime('%H:%M:%S.%l', time.gmtime(end - start))}\")\n",
    "print(f\"In seconds: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, *args):\n",
    "    pretty_print_result(es.search(index='myandex', body=query, size=100), args)\n",
    "    # note that size set to 20 just because default value is 10 and we know that we have 12 docs and 10 < 12 < 20\n",
    "\n",
    "def raw_search(query):\n",
    "    search_result = es.search(index='myandex', body=query, size=100)['hits']\n",
    "    return [(hit['_id'], hit['_score']) for hit in search_result['hits']]\n",
    "    \n",
    "def pretty_print_result(search_result, fields=[]):\n",
    "    # fields is a list of fields names which we want to be printed\n",
    "    res = search_result['hits']\n",
    "    print(f'Total documents: {res[\"total\"][\"value\"]}')\n",
    "    for hit in res['hits'][:6]:\n",
    "        print(f'Doc {hit[\"_id\"]}, score is {hit[\"_score\"]}')\n",
    "        for field in fields:\n",
    "            print(f'{field}: {hit[\"_source\"][field]}')\n",
    "                  \n",
    "def get_doc_by_id(doc_id):\n",
    "    return es.get(index='myandex', id=doc_id)['_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_query(query):\n",
    "    return {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'should': {\n",
    "                'match': {\n",
    "                    'content': query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "\n",
    "q = get_query('<meta http-equiv=\"Content-Type\" content=\"text/html; charset=windows-1251\">')\n",
    "search(q)\n",
    "raw_search(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries_and_relevance():\n",
    "    relevance = defaultdict(dict)\n",
    "    filename = '../relevant_table_2009.xml'\n",
    "    with open(filename, 'rb') as f:\n",
    "        xmldict = xmltodict.parse(f.read())\n",
    "        for task in tqdm(xmldict['taskDocumentMatrix']['task']):\n",
    "            task_rel = {}\n",
    "            has_vital = False\n",
    "            for doc in task['document']:\n",
    "                if doc['@relevance'] == 'vital':\n",
    "                    has_vital = True\n",
    "                task_rel[doc['@id']] = doc['@relevance']\n",
    "            if has_vital:\n",
    "                relevance[task['@id']] = task_rel\n",
    "    filename = '../web2008_adhoc.xml'\n",
    "    with open(filename, 'rb') as f:\n",
    "        xmldict = xmltodict.parse(f.read())\n",
    "        for task in tqdm(xmldict['task-set']['task']):\n",
    "            if task['@id'] in relevance:\n",
    "                relevance[task['@id']]['querytext'] = task['querytext']\n",
    "    return relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3cc3d5045c47ec9290caf1b1587339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb5b3fc329449babfad80f59f283d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=29231), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "relevance = load_queries_and_relevance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_number_of_correct_out_of_k(results, task_relevance, k):\n",
    "    return sum([1 if res[0] in task_relevance and task_relevance[res[0]] == 'vital' else 0 for res in results[:k]])\n",
    "\n",
    "def measure_performance():    \n",
    "    Q = len(relevance)\n",
    "    mrr = 0\n",
    "    ndcg_mean = 0\n",
    "    for task in relevance.keys():\n",
    "        task_relevance = relevance[task]\n",
    "        results = raw_search(get_query(task_relevance['querytext']))\n",
    "        idx = get_rank_indicies(results)\n",
    "        results = results[idx]\n",
    "        reciprocal_rank = 0\n",
    "        ndcg = 0\n",
    "        for i, res in enumerate(results):\n",
    "            rel_i = 0\n",
    "            if res[0] in task_relevance and task_relevance[res[0]] == 'vital':\n",
    "                reciprocal_rank = 1 / (i + 1)\n",
    "                rel_i = 1\n",
    "            ndcg += rel_i / (np.log2(i + 2))\n",
    "        normalizer = 0\n",
    "        for i in range(get_number_of_correct_out_of_k(results, task_relevance, 100)):\n",
    "            normalizer += 1 / (np.log2(i + 2))\n",
    "        ndcg_mean += ndcg / normalizer if normalizer != 0 else 0\n",
    "        mrr += reciprocal_rank\n",
    "    print(f\"ndcg@100: {mrr / Q}\")\n",
    "    print(f\"mrr@100: {ndcg_mean / Q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank_indicies(docs):\n",
    "    #TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "import xmltodict\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_doc_id_to_url():\n",
    "    docs_id_url_file_name = \"./pagerank.txt\"\n",
    "    id_to_url = defaultdict(str)\n",
    "\n",
    "    with open(docs_id_url_file_name) as docs_id_url_file:\n",
    "        for line in tqdm(docs_id_url_file):\n",
    "            tokens = line.split(\" \")\n",
    "            id_to_url[tokens[0]] = tokens[1]\n",
    "\n",
    "    return id_to_url\n",
    "\n",
    "\n",
    "def get_all_docs_for_train():\n",
    "    docs_dir = \"../../lemmatized_titles_pr_len\"\n",
    "    print(\"Getting doc id to url...\")\n",
    "    id_to_url = get_doc_id_to_url()\n",
    "    all_docs = defaultdict(dict)\n",
    "\n",
    "    print(\"Walking in docs dir...\")\n",
    "    for _, _, files in os.walk(docs_dir):\n",
    "        for doc_filename in tqdm(files):\n",
    "            try:\n",
    "                with open(docs_dir + \"/\" + doc_filename, encoding='utf-8') as doc_file:\n",
    "                    doc_id = doc_filename[4:]\n",
    "                    doc_url = id_to_url[doc_id]\n",
    "                    doc = json.load(doc_file)\n",
    "\n",
    "                    doc_dict = defaultdict(str)\n",
    "                    doc_dict[\"id\"] = doc_id\n",
    "                    doc_dict[\"url\"] = doc_url\n",
    "                    doc_dict[\"title\"] = doc[\"title\"]\n",
    "                    doc_dict[\"pagerank\"] = doc[\"pagerank\"]\n",
    "                    doc_dict[\"urllen\"] = doc[\"urllen\"]\n",
    "                    doc_dict[\"doclen\"] = doc[\"doclen\"]\n",
    "                    doc_dict[\"content\"] = doc[\"content\"]\n",
    "\n",
    "                    all_docs[doc_url] = doc_dict\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "def get_all_docs_for_test():\n",
    "    docs_dir = \"../../lemmatized_titles_pr_len\"\n",
    "    id_to_url = get_doc_id_to_url()\n",
    "    all_docs = defaultdict(dict)\n",
    "\n",
    "    for _, _, files in os.walk(docs_dir):\n",
    "        for doc_filename in tqdm(files):\n",
    "            try:\n",
    "                with open(docs_dir + \"/\" + doc_filename, encoding='utf-8') as doc_file:\n",
    "                    doc_id = doc_filename[4:]\n",
    "                    doc_url = id_to_url[doc_id]\n",
    "                    doc = json.load(doc_file)\n",
    "\n",
    "                    doc_dict = defaultdict(str)\n",
    "                    doc_dict[\"id\"] = doc_id\n",
    "                    doc_dict[\"url\"] = doc_url\n",
    "                    doc_dict[\"title\"] = doc[\"title\"]\n",
    "                    doc_dict[\"pagerank\"] = doc[\"pagerank\"]\n",
    "                    doc_dict[\"urllen\"] = doc[\"urllen\"]\n",
    "                    doc_dict[\"doclen\"] = doc[\"doclen\"]\n",
    "                    doc_dict[\"content\"] = doc[\"content\"]\n",
    "\n",
    "                    all_docs[doc_id] = doc_dict\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "def get_all_queries():\n",
    "    queries_filename = \"./web2008_adhoc.xml\"\n",
    "    all_queries = defaultdict(tuple)\n",
    "\n",
    "    with open(queries_filename, encoding='cp1251') as queries_file:\n",
    "        xml_dict = xmltodict.parse(queries_file.read())\n",
    "        for task in tqdm(xml_dict['task-set']['task']):\n",
    "            all_queries[task['@id']] = (task['@id'][3:], task['querytext'])\n",
    "\n",
    "    return all_queries\n",
    "\n",
    "\n",
    "def get_train_query_doc_pairs():\n",
    "    relevant_table_filename = \"./or_relevant-minus_table.xml\"\n",
    "\n",
    "    print(\"Getting all docs...\")\n",
    "    all_docs = get_all_docs_for_train()\n",
    "    print(\"Getting all queries...\")\n",
    "    all_queries = get_all_queries()\n",
    "    query_doc_pairs = []\n",
    "\n",
    "    print(\"Calculating features...\")\n",
    "    with open(relevant_table_filename) as table_file:\n",
    "        xml_dict = xmltodict.parse(table_file.read())\n",
    "\n",
    "        for query_dict in tqdm(xml_dict['taskDocumentMatrix']['task']):\n",
    "            try:\n",
    "                query_id = query_dict['@id']\n",
    "                query = all_queries[query_id]\n",
    "\n",
    "                for doc_dict in query_dict['document']:\n",
    "                    doc_url = doc_dict['@id']\n",
    "                    relevance_str = doc_dict['@relevance']\n",
    "                    relevance = 1 if relevance_str == \"vital\" else 0\n",
    "                    doc = all_docs[doc_url]\n",
    "                    query_doc_pairs.append((query, doc, relevance))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    return query_doc_pairs\n",
    "\n",
    "\n",
    "def get_test_query_doc_pairs(doc_ids):\n",
    "    relevant_table_filename = \"./relevant_table_2009.xml\"\n",
    "\n",
    "    all_docs = get_all_docs_for_test()\n",
    "    all_queries = get_all_queries()\n",
    "    query_doc_pairs = []\n",
    "\n",
    "    with open(relevant_table_filename) as table_file:\n",
    "        xml_dict = xmltodict.parse(table_file.read())\n",
    "\n",
    "        for query_dict in tqdm(xml_dict['taskDocumentMatrix']['task']):\n",
    "            try:\n",
    "                query_id = query_dict['@id']\n",
    "                print(query_id)\n",
    "                query = all_queries[query_id]\n",
    "\n",
    "                for doc_dict in query_dict['document']:\n",
    "                    doc_id = doc_dict['@id']\n",
    "                    if doc_id not in doc_ids[query_id]:\n",
    "                        continue\n",
    "                    relevance_str = doc_dict['@relevance']\n",
    "                    relevance = 1 if relevance_str == \"vital\" else 0\n",
    "                    doc = all_docs[doc_id]\n",
    "                    query_doc_pairs.append((query, doc, relevance))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    return query_doc_pairs\n",
    "\n",
    "\n",
    "def build_features(doc_ids):\n",
    "    query_doc_pairs = []\n",
    "\n",
    "    print(\"Getting query doc pairs...\")\n",
    "    dataset_type = \"test\"\n",
    "    if dataset_type == \"train\":\n",
    "        query_doc_pairs = get_train_query_doc_pairs()\n",
    "    elif dataset_type == \"test\":\n",
    "        query_doc_pairs = get_test_query_doc_pairs(doc_ids)\n",
    "\n",
    "    out_filename = dataset_type + \"_generated_features_elastic.txt\"\n",
    "\n",
    "    with open(out_filename, \"w\") as out_file:\n",
    "        for query, doc, relevance in tqdm(query_doc_pairs):\n",
    "            try:\n",
    "                features = []\n",
    "                # calculate new_feature_value\n",
    "                # features.append(new_feature_value)\n",
    "                features.append(len(query[1]))\n",
    "                features.append(doc[\"urllen\"])\n",
    "                features.append(doc[\"doclen\"])\n",
    "                features.append(doc[\"pagerank\"])\n",
    "\n",
    "                out_file.write(str(relevance) + \" \")\n",
    "                out_file.write(\"quid:\" + query[0] + \" \")\n",
    "                for i, feature in enumerate(features, start=1):\n",
    "                    out_file.write(str(i) + \":\" + str(feature) + \" \")\n",
    "                out_file.write(\"\\n\")\n",
    "            except Exception as e:\n",
    "                print(\"Exception: \")\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance_2():    \n",
    "    Q = len(relevance)\n",
    "    mrr = 0\n",
    "    ndcg_mean = 0\n",
    "    doc_ids = {}\n",
    "    for task in relevance.keys():\n",
    "        task_relevance = relevance[task]\n",
    "        doc_ids[task] = [pair[0] for pair in raw_search(get_query(task_relevance['querytext']))][:100]\n",
    "    build_features(doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75977it [00:00, 759767.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting query doc pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199202it [00:00, 747846.67it/s]\n",
      "100%|██████████| 29231/29231 [00:00<00:00, 817695.98it/s]\n",
      "100%|██████████| 547/547 [00:00<00:00, 9120.73it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arw57797\n",
      "arw53809\n",
      "arw57272\n",
      "arw53808\n",
      "arw57796\n",
      "arw53806\n",
      "arw51174\n",
      "arw53801\n",
      "arw50554\n",
      "arw49893\n",
      "arw53534\n",
      "arw49692\n",
      "arw58427\n",
      "arw56522\n",
      "arw53743\n",
      "arw52490\n",
      "arw59004\n",
      "arw58530\n",
      "arw53207\n",
      "arw50642\n",
      "arw51183\n",
      "arw50640\n",
      "arw53202\n",
      "arw51903\n",
      "arw50548\n",
      "arw52682\n",
      "arw49983\n",
      "arw57994\n",
      "arw59134\n",
      "arw53738\n",
      "arw50925\n",
      "arw58439\n",
      "arw52920\n",
      "arw52544\n",
      "'arw52544'\n",
      "arw53549\n",
      "'arw53549'\n",
      "arw53730\n",
      "arw52924\n",
      "arw51197\n",
      "arw52672\n",
      "arw56405\n",
      "'arw56405'\n",
      "arw56407\n",
      "arw56202\n",
      "arw52478\n",
      "arw56409\n",
      "arw58540\n",
      "'arw58540'\n",
      "arw50530\n",
      "arw54686\n",
      "arw53317\n",
      "arw52379\n",
      "arw52570\n",
      "arw57298\n",
      "arw58403\n",
      "arw53926\n",
      "arw53823\n",
      "arw52912\n",
      "arw52578\n",
      "arw52373\n",
      "'arw52373'\n",
      "arw54689\n",
      "arw54481\n",
      "arw52678\n",
      "arw53726\n",
      "arw52472\n",
      "'arw52472'\n",
      "arw50122\n",
      "'arw50122'\n",
      "arw52470\n",
      "arw50529\n",
      "'arw50529'\n",
      "arw52662\n",
      "arw53913\n",
      "arw49782\n",
      "arw53029\n",
      "arw57161\n",
      "arw54690\n",
      "arw53811\n",
      "'arw53811'\n",
      "arw52561\n",
      "arw53610\n",
      "arw53320\n",
      "'arw53320'\n",
      "arw54499\n",
      "arw54498\n",
      "arw54495\n",
      "arw53718\n",
      "arw50276\n",
      "arw52568\n",
      "arw56218\n",
      "arw50280\n",
      "arw51835\n",
      "'arw51835'\n",
      "arw53844\n",
      "arw52592\n",
      "arw50695\n",
      "arw50892\n",
      "arw53700\n",
      "arw50384\n",
      "arw50690\n",
      "'arw50690'\n",
      "arw49936\n",
      "arw58766\n",
      "arw53701\n",
      "'arw53701'\n",
      "arw50972\n",
      "arw49938\n",
      "arw59044\n",
      "arw59045\n",
      "arw50292\n",
      "arw50498\n",
      "arw51944\n",
      "arw53305\n",
      "arw52446\n",
      "arw50197\n",
      "'arw50197'\n",
      "arw49944\n",
      "arw51949\n",
      "arw51844\n",
      "arw56439\n",
      "arw53977\n",
      "arw52448\n",
      "'arw52448'\n",
      "arw50191\n",
      "arw51080\n",
      "arw50882\n",
      "arw57561\n",
      "'arw57561'\n",
      "arw57564\n",
      "arw50885\n",
      "arw53046\n",
      "arw50491\n",
      "arw53589\n",
      "arw54474\n",
      "arw49662\n",
      "arw50495\n",
      "arw57193\n",
      "arw50577\n",
      "arw54446\n",
      "arw53867\n",
      "arw49950\n",
      "arw54589\n",
      "arw54588\n",
      "arw53454\n",
      "arw49953\n",
      "arw57198\n",
      "arw49958\n",
      "'arw49958'\n",
      "arw56440\n",
      "arw50992\n",
      "arw53457\n",
      "arw57573\n",
      "arw54581\n",
      "'arw54581'\n",
      "arw49674\n",
      "arw57399\n",
      "arw54584\n",
      "arw58642\n",
      "arw50570\n",
      "arw52331\n",
      "arw50778\n",
      "arw53563\n",
      "arw57972\n",
      "arw59118\n",
      "arw54459\n",
      "arw56558\n",
      "arw54419\n",
      "'arw54419'\n",
      "arw59116\n",
      "arw54599\n",
      "arw52427\n",
      "arw51823\n",
      "arw50981\n",
      "arw59015\n",
      "arw54453\n",
      "arw50763\n",
      "arw51061\n",
      "arw50761\n",
      "arw50764\n",
      "arw58656\n",
      "arw55712\n",
      "arw53070\n",
      "'arw53070'\n",
      "arw55711\n",
      "arw56467\n",
      "arw50340\n",
      "arw53079\n",
      "arw52410\n",
      "arw58729\n",
      "arw57089\n",
      "arw50443\n",
      "arw51707\n",
      "arw53885\n",
      "arw55514\n",
      "arw58635\n",
      "arw59071\n",
      "arw58328\n",
      "'arw58328'\n",
      "arw58633\n",
      "arw59075\n",
      "arw56865\n",
      "arw58731\n",
      "arw57724\n",
      "arw58737\n",
      "arw50454\n",
      "arw52215\n",
      "arw53876\n",
      "arw55524\n",
      "arw57072\n",
      "arw53874\n",
      "arw53873\n",
      "arw59081\n",
      "arw57738\n",
      "arw49917\n",
      "arw57535\n",
      "arw57730\n",
      "'arw57730'\n",
      "arw57733\n",
      "arw58904\n",
      "arw58355\n",
      "arw53699\n",
      "arw56597\n",
      "'arw56597'\n",
      "arw56686\n",
      "arw55609\n",
      "arw56785\n",
      "arw49924\n",
      "'arw49924'\n",
      "arw58751\n",
      "arw56982\n",
      "arw50334\n",
      "'arw50334'\n",
      "arw58206\n",
      "arw57741\n",
      "arw55504\n",
      "arw56882\n",
      "arw58344\n",
      "arw58914\n",
      "arw57647\n",
      "arw52100\n",
      "arw50104\n",
      "arw52658\n",
      "arw50201\n",
      "arw56630\n",
      "arw56631\n",
      "arw50109\n",
      "arw52242\n",
      "arw52248\n",
      "arw56082\n",
      "arw55753\n",
      "arw56639\n",
      "arw57618\n",
      "arw56634\n",
      "arw57752\n",
      "arw55895\n",
      "arw55894\n",
      "arw53638\n",
      "arw52503\n",
      "arw53636\n",
      "arw53506\n",
      "arw57620\n",
      "arw57760\n",
      "arw56232\n",
      "arw56192\n",
      "arw59172\n",
      "arw55639\n",
      "arw52120\n",
      "arw53765\n",
      "arw50227\n",
      "arw50914\n",
      "arw56345\n",
      "arw52227\n",
      "arw56656\n",
      "arw53660\n",
      "arw55770\n",
      "arw57871\n",
      "'arw57871'\n",
      "arw52125\n",
      "arw57069\n",
      "'arw57069'\n",
      "arw56068\n",
      "arw56247\n",
      "'arw56247'\n",
      "arw56741\n",
      "arw56742\n",
      "arw52130\n",
      "arw57502\n",
      "arw53754\n",
      "'arw53754'\n",
      "arw53753\n",
      "arw57601\n",
      "arw58105\n",
      "arw58309\n",
      "arw58304\n",
      "arw52239\n",
      "arw56645\n",
      "arw50231\n",
      "arw55786\n",
      "arw52235\n",
      "arw57780\n",
      "arw57602\n",
      "arw57881\n",
      "arw52135\n",
      "arw57503\n",
      "arw57056\n",
      "'arw57056'\n",
      "arw58512\n",
      "arw58510\n",
      "arw51775\n",
      "arw55076\n",
      "arw55541\n",
      "arw56058\n",
      "arw56056\n",
      "arw54097\n",
      "arw55347\n",
      "arw56155\n",
      "arw57705\n",
      "arw54094\n",
      "arw56153\n",
      "arw57701\n",
      "arw57704\n",
      "arw55684\n",
      "arw51672\n",
      "arw58128\n",
      "arw52171\n",
      "arw56840\n",
      "arw55549\n",
      "arw55274\n",
      "arw55279\n",
      "arw54295\n",
      "arw55989\n",
      "'arw55989'\n",
      "arw52033\n",
      "arw55089\n",
      "arw54087\n",
      "arw56043\n",
      "arw56143\n",
      "arw54081\n",
      "arw55673\n",
      "arw56717\n",
      "'arw56717'\n",
      "arw51683\n",
      "arw56913\n",
      "arw58136\n",
      "arw55143\n",
      "arw56916\n",
      "'arw56916'\n",
      "arw56915\n",
      "arw51780\n",
      "'arw51780'\n",
      "arw52021\n",
      "arw55289\n",
      "arw52024\n",
      "arw57015\n",
      "'arw57015'\n",
      "arw56173\n",
      "arw57016\n",
      "arw56036\n",
      "arw56728\n",
      "arw50017\n",
      "arw56724\n",
      "arw55155\n",
      "'arw55155'\n",
      "arw51519\n",
      "arw58974\n",
      "arw58008\n",
      "arw58970\n",
      "arw51510\n",
      "arw56834\n",
      "arw58299\n",
      "arw56839\n",
      "arw55066\n",
      "arw58296\n",
      "arw58295\n",
      "arw52796\n",
      "arw57417\n",
      "arw52799\n",
      "arw56735\n",
      "arw52790\n",
      "arw56937\n",
      "'arw56937'\n",
      "arw55856\n",
      "arw55168\n",
      "arw55169\n",
      "arw55859\n",
      "arw52791\n",
      "arw55557\n",
      "arw55559\n",
      "arw52040\n",
      "arw58015\n",
      "arw55998\n",
      "arw55999\n",
      "arw51738\n",
      "arw55448\n",
      "arw53591\n",
      "arw53593\n",
      "arw53594\n",
      "arw58367\n",
      "arw56114\n",
      "arw58265\n",
      "arw54056\n",
      "arw56805\n",
      "arw58958\n",
      "arw58362\n",
      "arw55032\n",
      "arw56015\n",
      "arw56801\n",
      "'arw56801'\n",
      "arw54055\n",
      "arw54193\n",
      "arw52075\n",
      "arw51531\n",
      "arw52072\n",
      "arw57803\n",
      "'arw57803'\n",
      "arw57802\n",
      "arw51324\n",
      "arw57947\n",
      "arw51435\n",
      "arw56948\n",
      "arw51644\n",
      "'arw51644'\n",
      "arw58378\n",
      "arw54993\n",
      "arw58279\n",
      "arw51741\n",
      "'arw51741'\n",
      "arw55577\n",
      "'arw55577'\n",
      "arw55045\n",
      "arw58809\n",
      "arw54703\n",
      "arw51448\n",
      "arw58075\n",
      "arw57932\n",
      "arw50064\n",
      "'arw50064'\n",
      "arw58074\n",
      "arw58177\n",
      "'arw58177'\n",
      "arw56131\n",
      "arw54176\n",
      "arw51550\n",
      "arw54962\n",
      "arw58930\n",
      "arw58183\n",
      "arw57829\n",
      "arw52094\n",
      "arw55963\n",
      "arw52095\n",
      "arw55421\n",
      "arw58832\n",
      "arw55696\n",
      "arw55697\n",
      "arw54165\n",
      "arw58828\n",
      "arw55590\n",
      "'arw55590'\n",
      "arw58392\n",
      "arw58258\n",
      "arw58927\n",
      "arw51761\n",
      "arw55023\n",
      "arw55029\n",
      "arw52080\n",
      "arw55811\n",
      "arw58051\n",
      "arw52186\n",
      "arw51424\n",
      "arw50040\n",
      "arw55815\n",
      "arw57208\n",
      "arw54553\n",
      "arw54757\n",
      "arw50846\n",
      "arw54949\n",
      "arw51477\n",
      "arw54854\n",
      "arw55807\n",
      "arw54016\n",
      "arw51888\n",
      "arw54217\n",
      "arw54215\n",
      "arw52825\n",
      "arw49720\n",
      "arw57204\n",
      "'arw57204'\n",
      "arw49723\n",
      "arw55008\n",
      "arw54841\n",
      "arw54843\n",
      "arw50850\n",
      "arw57471\n",
      "arw51580\n",
      "arw55205\n",
      "arw54646\n",
      "arw52834\n",
      "arw54205\n",
      "arw51157\n",
      "arw52736\n",
      "arw53494\n",
      "arw54734\n",
      "arw51349\n",
      "arw51343\n",
      "arw49707\n",
      "arw50728\n",
      "arw54670\n",
      "arw54238\n",
      "arw58495\n",
      "arw49843\n",
      "arw51242\n",
      "arw58091\n",
      "arw53482\n",
      "arw53480\n",
      "arw54134\n",
      "arw54539\n",
      "arw52980\n",
      "arw58499\n",
      "'arw58499'\n",
      "arw52998\n",
      "arw51332\n",
      "arw54521\n",
      "arw49633\n",
      "arw49838\n",
      "arw50871\n",
      "arw51035\n",
      "arw54862\n",
      "arw51855\n",
      "arw54026\n",
      "arw50082\n",
      "arw51859\n",
      "arw51139\n",
      "'arw51139'\n",
      "arw54124\n",
      "arw57216\n",
      "arw51994\n",
      "arw53475\n",
      "arw54391\n",
      "arw52760\n",
      "arw54908\n",
      "arw56611\n",
      "arw53263\n",
      "arw57446\n",
      "arw52263\n",
      "arw51121\n",
      "arw57445\n",
      "arw54795\n",
      "arw57449\n",
      "arw57448\n",
      "arw51127\n",
      "arw53269\n",
      "arw52863\n",
      "arw57148\n",
      "arw50507\n",
      "arw50602\n",
      "arw54895\n",
      "arw54616\n",
      "'arw54616'\n",
      "arw51261\n",
      "arw52975\n",
      "'arw52975'\n",
      "arw54385\n",
      "arw56607\n",
      "arw54507\n",
      "arw57131\n",
      "arw52970\n",
      "arw51113\n",
      "'arw51113'\n",
      "arw52278\n",
      "arw55108\n",
      "arw54604\n",
      "arw54882\n",
      "arw54881\n",
      "arw57325\n",
      "arw55115\n",
      "arw57463\n",
      "arw49745\n",
      "arw53247\n",
      "arw51494\n",
      "'arw51494'\n",
      "arw49740\n",
      "arw52745\n",
      "arw51002\n",
      "arw54639\n",
      "arw52883\n",
      "arw50623\n",
      "arw55492\n",
      "arw54264\n",
      "arw55221\n",
      "arw54266\n",
      "arw55393\n",
      "arw55495\n",
      "arw49731\n",
      "arw54368\n",
      "arw54628\n",
      "arw53136\n",
      "arw50632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "measure_performance_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('396447', 2.398802)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_search(get_query(\"я\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "def print_feature_importance():\n",
    "    train = './train_generated_features.txt'\n",
    "    with open(train) as train_features:\n",
    "        lines = train_features.readlines()\n",
    "        X = np.zeros((len(lines), 4))\n",
    "        y = np.zeros(len(lines))\n",
    "        for i, line in enumerate(lines):\n",
    "            parts = line.split(\"\\n\")[0].split(\" \")\n",
    "            X[i][0] = float(parts[2].split(\":\")[1])\n",
    "            X[i][1] = float(parts[3].split(\":\")[1])\n",
    "            X[i][2] = float(parts[4].split(\":\")[1])\n",
    "            X[i][3] = float(parts[5].split(\":\")[1])\n",
    "            y[i] = parts[0]\n",
    "    print(mutual_info_classif(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0336692   0.00565359  0.02178088  0.04768679]\n"
     ]
    }
   ],
   "source": [
    "print_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_homeworks)",
   "language": "python",
   "name": "ml_homeworks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
